---
title: "Prediction on Readmission to Hospital using a Popular Diabetes Dataset"
author: "Yijia Liu"
date: "December 22, 2020"
bibliography: reference.bib
fontsize: 12pt
linkcolor: blue
output:
  pdf_document: default
  html_document:
    df_print: paged
biblio-style: alphadin
link-citations: true
abstract: "It is important to know if a diabetes patient will be readmitted in some hospital. The reason is that you can change the treatment, in order to avoid a readmission. We apply Generalized linear model(GLM) [@nelder1972generalized], Generalized Linear Mixed Models (GLMMs) [@mcculloch2014generalized] and Backward Selection [@hocking1976biometrics] to select variables and decide the finalized model. Finally, 10 variables are found to help predict readmission rate and the prediction accuracy is around 0.88, which is an accept result. \\par
 \\textbf{Keywords:} Diabetes, Readmission, GLM, AIC, BIC \\par
 \\textbf{Data:} https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008 \\par
 \\textbf{Github:} https://github.com/LesleyyijiaLiu/Prediction-on-Readmission-to-Hospital-using-a-Popular-Diabetes-Dataset.git"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# install packages

library(MASS) 
library(car) 
library(lmtest) 
library(lme4)
library(tidyverse) 
library(magrittr) 
library(qqtest) 

```


# 1. Introduction

Diabetes is a life long condition where your body does not produce enough insulin or your body cannot use the insulin it has effectively. It can increase the risk of high blood pressure, narrowing of the arteries (atherosclerosis), coronary artery disease and stroke.[@diabetes2019] Thus, it is important to identify the patients with worse outcomes.
In general, it is hard to measure directly. However, readmission can provide a low cost, stable estimate. If the readmission are none, which means the treatment is great; else if the readmission are less than 30 days, which means the treatment may not be appropriate; else if the readmission are more than 30 days, which means the treatment is not very good, but the reason could be the patients.

In this project, we would like to make prediction on 'readmission' using the variables from the dataset. Firstly, we recombine 'readmission' to a binary variable and apply 'GLM' model to find a linear relationship. Secondly, variable selection is applied to find more efficient variables. Thirdly, we construct a better model with conditioning on 'patient_nbr'. Finally, model validation and prediction testing are made to make the project more convincing.

The structure of this project is: we simply describe the data and explain the variables firstly, and then give an introduction of our model. After that, the procedures of getting our final model will be provided. In the end, we will talk about the prediction results and discussion parts.

# 2. Data

## 2.1. Source of the Data

The data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. We find this data set from Kaggle but the raw data are submitted on behalf of the Center for Clinical and Transnational Research, Virginia Commonwealth University, a recipient of NIH CTSA grant UL1 TR00058 and a recipient of the CERNER data. John Clore, Krzysztof J. Cios, Jon DeShazo, and Beata Strack. This data is a de-identified abstract of the Health Facts database[@datasource].

It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.

- It is an inpatient encounter (a hospital admission). 

- It is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis. 

- The length of stay was at least 1 day and at most 14 days. 

- Laboratory tests were performed during the encounter. 

- Medications were administered during the encounter. 

The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.

## 2.2. Description of the Data

The data contains 101766 observations from 71518 patients. Some of the 50 features representing patient and hospital outcomes are numerical and some are categorical. 

- Encounter ID: Unique identifier of an encounter
- Patient number: Unique identifier of a patient
- Race Values: Caucasian, Asian, African American, Hispanic, and other
- Gender Values: male, female, and unknown/invalid
- Age Grouped in 10 -year intervals: 0,10$), 10,20), \ldots, 90,100)$
- Weight: Weight in pounds
- Admission type: Integer identifier corresponding to 9 distinct values, for example,
emergency, urgent, elective, newborn, and not available
- Discharge disposition: Integer identifier corresponding to 29 distinct values, for
example, discharged to home, expired, and not available
- Admission source: Integer identifier corresponding to 21 distinct values, for
example, physician referral, emergency room, and transfer from a hospital
- Time in hospital: Integer number of days between admission and discharge
- Payer code: Integer identifier corresponding to 23 distinct values, for example, Blue
Cross/Blue Shield, Medicare, and self-pay Medical
- Medical specialty: Integer identifier of a specialty of the admitting physician,
corresponding to 84 distinct values, for example, cardiology, internal medicine,
family/general practice, and surgeon
- Number of medications: Number of distinct generic names administered during the
encounter
- Number of outpatient visits: Number of outpatient visits of the patient in the year
preceding the encounter
- Number of emergency visits: Number of emergency visits of the patient in the year
preceding the encounter
- Number of inpatient visits: Number of inpatient visits of the patient in the year
preceding the encounter
- Diagnosis 1: The primary diagnosis (coded as first three digits of ICD9); 848 distinct
values
- Diagnosis 2: Secondary diagnosis (coded as first three digits of ICD9); 923 distinct
values
- Diagnosis 3: Additional secondary diagnosis (coded as first three digits of ICD9);
954 distinct values
- Number of diagnoses: Number of diagnoses entered to the system $0 \%$
- Glucose serum test result: Indicates the range of the result or if the test was not
taken. Values: $^{\prime \prime}>200, "^{\prime \prime}>300,$ " "normal," and "none" if not measured
- A1c test result: Indicates the range of the result or if the test was not taken. Values:
$">8 "$ if the result was greater than $8 \%,{ }^{\prime \prime}>7^{\prime \prime}$ if the result was greater than $7 \%$ but
less than $8 \%,$ "normal" if the result was less than $7 \%,$ and "none" if not measured.
- Change of medications: Indicates if there was a change in diabetic medications
(either dosage or generic name). Values: "change" and "no change"
- Diabetes medications: Indicates if there was any diabetic medication prescribed.
Values: "yes" and "no"
- 24 features for medications For the generic names: metformin, repaglinide,
nateglinide, chlorpropamide, glimepiride, acetohexamide, glipizide, glyburide,
tolbutamide, pioglitazone, rosiglitazone, acarbose, miglitol, troglitazone, tolazamide, examide, sitagliptin, insulin, glyburide-metformin, glipizide-metformin, glimepiride- pioglitazone, metformin-rosiglitazone, and metformin-pioglitazone, the feature indicates whether the drug was prescribed or there was a change in the dosage. Values: "up" if the dosage was increased during the encounter, "down" if the dosage was decreased, "steady" if the dosage did not change, and "no" if the drug was not prescribed
- Readmitted: Days to inpatient readmission. Values: $^{\prime \prime}<30^{\prime \prime}$ if the patient was readmitted in less than 30 days, $^{\prime \prime}>30^{\prime \prime}$ if the patient was readmitted in more than 30 days, and "No" for no record of readmission

## 2.3. Manipulation of the Data

To clean the data and keep the most important variables, we will do the following steps:

(1)	To the response variable (readmission), since the distribution is imbalanced (11357 less than 30s, 35545 over 30s and 54864 no), additionally, the distinction of the less than 30 readmission and over 30 readmission is not great, we decide to use two levels that are 'no readmission' and 'readmission'.
Thus, we transform the response variable 'readmission' to be binary: combine 'less than 30 (total 11357)' and 'over 30 (total 35545)' to '1', which means the patient does have readmission; change 'no' to '0 (54864)', means there is no readmission. This transformation also helps keep the data balanced.

(2)	Remove some seriously imbalanced or NA variables: Since for 'weights', 'payer_code' and 'medical_specialty' contain almost all NAs, we will remove them. Also, 'examide', 'metformin_rosiglitazone' and 'citoglipton' are removed due to the seriously imbalanced.

(3)	Remove other rows which contains NAs.

(4)	Correlation Matrix: For all rest numerical variables, we can construct the correlation matrix as following(Figure 1):

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

df = read.csv("diabetes.csv")
# summary(df)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# boxplot to find out the basic statistical description
# boxplot(df)

library("dplyr")
# correlation coefficient matrix
df_noclass = select_if(df, is.numeric)
df.cor = cor(df_noclass, method = c("spearman"))
# df_noclass has only 15 columns
# df.cor

# Correlation Visualization
library(GGally)
df_noclass %>% 
  select(!contains("X")) %>%
  ggcorr(size = 3, nudge_x = -1) + 
  ggtitle("Correlation matrix") + 
  labs(caption = "Figure 1: Correlation Matrix")

```

From the figure above, we can find 'num_medications' has relatively strong relationship with several variables and 'number_inpatient' has strong relationship with 'encounter_num'.

# 3. Models

## 3.1. Introduction to Models

Our model is built using `R` [@citeR], with packages `MASS` [@citeMASS], `car` [@citecar], `lmtest` [@citelmtest], `lme4` [@citelme4], `tidyverse` [@citetidyverse], `magrittr` [@citemagrittr], and `qqtest` [@citeqqtest].

### Model1: GLM with Binomial Response

When the response variable has only two outcomes, which follows a binomial
distribution $\operatorname{Bin}\left(m_{i}, \pi_{i}\right),$ we can use

$$
\mathrm{P}\left(Y_{i}=y_{i}\right)=\left(\begin{array}{c}
m_{i} \\
y_{i}
\end{array}\right) \pi_{i}^{y_{i}}\left(1-\pi_{i}\right)^{m_{i}-y_{i}}
$$

to express. We further assume that the $Y_{i}$ are independent. The individual trials that
compose $Y_{i}$ are subject to the same $q$ predictors $\left(x_{i 1}, \ldots, x_{i q}\right)$
As in the binary case, we construct a linear predictor

$$
\eta_{i}=\beta_{0}+\beta_{1} x_{i 1}+\cdots+\beta_{q} x_{i q}
$$

We can use a logistic link function $\eta_{i}=\log \left(\pi_{i} /\left(1-\pi_{i}\right)\right)$ and the log-likelihood is given by

$$
\ell(\beta)=\sum_{i=1}^{n}\left[y_{i} \eta_{i}-m_{i} \log \left(1+e^{\eta_{i}}\right)+\log \left(\begin{array}{c}
m_{i} \\
y_{i}
\end{array}\right)\right]
$$

The deviance is given by:
$$
D=2 \sum_{i=1}^{n}\left[\frac{y_{i} \log \left(y_{i}\right)}{\hat{y}_{i}}+\frac{\left(m_{i}-y_{i}\right) \log \left(m_{i}-y_{i}\right)}{m_{i}-\hat{y}_{i}}\right]
$$

### Model2: Generalized linear Mixed Models (GLMMs)

After considering random effects, we can use generalized linear mixed models. The interpretation of GLMMs is similar to GLMs; however, there is an added complexity because of the random effects. On the linear metric (after taking the link function), interpretation continues as usual. However, it is often easier to back transform the results to the original metric. For example, in a random effects logistic model, one might want to talk about the probability of an event given some specific values of the predictors. Likewise in a Poisson (count) model, one might want to talk about the expected count rather than the expected log count. These transformations complicate matters because they are nonlinear and so even random intercepts no longer play a strictly additive role and instead can have a multiplicative effect. When the response follows an exponential family distribution, 

$$
f\left(y_{i} \mid \theta_{i}, \phi\right)=\exp \left\{\frac{y_{i} \theta_{i}-b\left(\theta_{i}\right)}{a(\phi)}+c(y, \phi)\right\}
$$
Let $\mathbb{E}\left(Y_{i}\right)=\mu_{i}$ and we connect it to the linear predictor $\eta_{i}$ using the link function $g$ by $\eta_{i}=g\left(\mu_{i}\right)$. If the random effect $\gamma$ have distribution $h(\gamma \mid V)$ for parameters
V. The fixed effects are $\beta$. Condition on the random effects $\gamma$, we have $\theta_{i}=x_{i}^{\top}+$ $z_{i}^{\top} \gamma .$ Then the likelihood function is
$$
L(\beta, \phi, V \mid y)=\prod_{i=1}^{n} \int f\left(y_{i} \mid \beta, \phi, \gamma\right) h(\gamma \mid V) d \gamma
$$
Where $\gamma \sim N(0, D)$

## 3.2. Variable Selection

Some variables in original model are redundant or there exist Multicollinearity. So I can develop some variable selection method:

### (a) AIC and BIC Criterion:

AIC [@sakamoto1986akaike] and BIC [@schwarz1978estimating] are Information criteria methods used to assess model fit while
penalizing the number of estimated parameters. Let $k$ be the number of estimated  parameters in the model. Let $L$ be the maximum value of the likelihood function
for the model. Then the AIC value of the model is the following.
$$
\mathrm{AIC}=2 k-2 \ln (L)
$$
The formula for (BIC) is similar to the formula for AIC, but with a different penalty for the number of parameters.
$$
\mathrm{BIC}=\ln (n) k-2 \ln (\mathrm{L})
$$

### (b) Stepwise selection

Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. 

- Forward selection: Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. 

- Backward selection: The backward selection model starts with all candidate variables in the model. At each step, the variable that is the least significant is removed. We applied backward selection model in my project.

## 3.3. Process of Obtaining Final Model

- (a) Split the data to train set and test set: I create a test dataset that contains a random selection of 20000 patients by using 'patient nbr' variable. The rest are set as train set.

- (b) Remove other variables which cannot be used as covariates: I continue to remove "encounter id", "patient nbr", "admission source id", "encounter num" before fitting models.

- (c) Fitting GLM with Binomial Response: since the response variable "readmitted" is a binary variable, we can let $Y_{i} \sim$ Bernoulli $\left(\pi_{i}\right),$ where $\pi_{i}$ is the probability of readmission. And then we fit a GLM as $\operatorname{logit}\left(\pi_{i}\right)=$ $X_{i} \beta+e_{i},$ where $e_{i} \sim N\left(0, \sigma^{2}\right)$

The fitted formula for model1 is: 

formula $=$ readmitted $\sim$ race $+$ gender $+$ age $+$ num lab procedures $+$ num procedures + num medications + number outpatient $+$ number emergency + number inpatient + number diagnoses $+$ max glu serum $+$ A1Cresult $+$ metformin $+$ glipizide $+$ pioglitazone $+$ rosiglitazone $+$ acarbose $+$ insulin $+$ change $+$ diabetesMed $+$ Length.of.Stay (totally 21 variables)

- (d) Backward Stepwise Selection: From the regression results above, I find half of the covariates are not significant. So I decide to use backward stepwise selection method to find more proper and simpler model but with reasonable explanation.

The final result is: 

readmitted $\sim$ race $+$ gender $+$ age $+$ num lab procedures $+$ num procedures + number outpatient + number emergency $+$ number inpatient + number diagnoses + max glu serum + A1Cresult + metformin $+$ glipizide $+$ rosiglitazone $+$ acarbose $+$ insulin $+$ diabetesMed + Length.of.Stay (totally 18 variables)

- (e) $\quad$ Generalized linear Mixed Model: After selecting variables using Stepwise, we still find some of the variables that are not significant. Since one patient may have more than once encounters, the generalized linear mixed model condition on "patient nar" maybe a better choice. We can let $Y_{i j} \mid U_{i} \sim$ Bernoulli $\left(\pi_{i t}\right)$ where $\pi_{i t}$ is the probability of readmission. And then we fit a GLMM as logit $\left(\pi_{i t}\right)=X_{i t} \beta+U_{i}$ 

The fitted formula for model 1 is: 

Formula: readmitted $\sim(1 \mid$ patient $\mathrm{nbr})+$ race $+$ gender $+$ age $+$ num lab procedures + num procedures + number outpatient $+$ number emergency + number inpatient + number diagnoses $+$
acarbose $+$ insulin $+$ diabetesMed $+$ Length.of.Stay

From the regression results, we continue to remove the variables are not significant, the final model is: 

final formula $=$ readmitted $\sim(1 \mid$ patient $\mathrm{nbr})+$ race $+$ gender $+$ age $+$ num procedures + number outpatient + number emergency $+$ number diagnoses $+$ insulin $+$ diabetesMed $+$ Length.of.Stay

## 3.4.	Model Validation/ Dignostics

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# read and delete NA variables and useless variables

df = read.csv("diabetes.csv") %>% 
  select(-c(X, weight, payer_code, medical_specialty, examide, citoglipton,
            metformin.rosiglitazone,
            acetohexamide, tolbutamide, troglitazone,
            glipizide.metformin, metformin.pioglitazone,
            glimepiride.pioglitazone)) %>% 
  # remove all NAs
  na.omit() %>% 
  # replace the response readmitted to binary variable
  mutate(
    readmitted = case_when(
      readmitted == "NO" ~ 0, readmitted != "NO" ~ 1 ))

# continue to remove 
# head(df)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# read and delete NA variables and useless variables

df = read.csv("diabetes.csv") %>% 
  select(-c(X, weight, payer_code, medical_specialty, examide, citoglipton,
            metformin.rosiglitazone,
            acetohexamide, tolbutamide, troglitazone,
            glipizide.metformin, metformin.pioglitazone,
            glimepiride.pioglitazone)) %>% 
  # remove all NAs
  na.omit() %>% 
  # replace the response readmitted to binary variable
  mutate(
    readmitted = case_when(
      readmitted == "NO" ~ 0, readmitted != "NO" ~ 1 ))

# continue to remove 
# head(df)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# split data to test set and train set

set.seed(12345)
unique_patient_id = unique(df$patient_nbr)
size = length(unique_patient_id) #(total 69668)
# 25% test set(total 16417) and 75% train set
sample_size = size*0.25
test_id = sample(unique_patient_id, size = sample_size)
train_id = setdiff(unique_patient_id, test_id)
# df_new = data.frame(df)
test_set = df[df$patient_nbr %in% test_id, ] # total 25017
train_set = df[df$patient_nbr %in% train_id, ] # total 74,476

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# remove other variables which cannot be used as covariates

new_train = subset(train_set, select=-c(encounter_id, patient_nbr, admission_source_id, encounter_num))

# originl model1

formula = readmitted ~ race + gender + age + num_lab_procedures + num_procedures + num_medications + number_outpatient + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glipizide + pioglitazone + rosiglitazone + acarbose + insulin + change + diabetesMed + Length.of.Stay

model = glm(formula, data = new_train, family = binomial)

# summary(model)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# Backward Select Variables for model1
# backward = step(model, direction = "backward")

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# After selecting variables, we try new model2

new_formula = readmitted ~ (1 | patient_nbr) + race + gender + age + num_lab_procedures + num_procedures + number_outpatient + number_emergency + number_inpatient + number_diagnoses + max_glu_serum + A1Cresult + metformin + glipizide + rosiglitazone + acarbose + insulin +  diabetesMed + Length.of.Stay

model2 = glmer(new_formula, data = train_set, family = binomial, nAGQ = 0)

# summary(model2)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# Backward Select Variables for model2

fixmodel <- lm(formula(model2, fixed.only=TRUE),
               data=eval(getCall(model2)$data))
# step(fixmodel)

```


```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# remove more variable according to statistical significance and correlation matrix

final_formula = readmitted ~ (1 | patient_nbr) + race + gender + age + num_procedures + number_outpatient + number_emergency + number_diagnoses + insulin +  diabetesMed + Length.of.Stay

model3 = glmer(final_formula, data = train_set, family = binomial, nAGQ = 0)

# summary(model3)

```

After selecting variables, we need to check the following assumptions:

- (a)  Normality of residuals(Figure 2); (QQ-plot)

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# Diagnostics
# 1. QQ-plot

residual = residuals(model3, "pearson", scaled = TRUE) 
tibble(residuals = residual) %>% ggplot(aes(sample = residuals)) + stat_qq() + stat_qq_line() + labs(caption = "Figure 2: QQ-Plot")
  
```

From the QQ plot above, we can find most of the points are on the straight line, which means the residuals follow normal distribution approximately.

- (b)  Homoscedasticity(Figure 3); (Fitted Values v.s. Residuals)

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# 2. fitted v.s. residuals

diagd = tibble(resid = residuals(model3), fitted = fitted(model3))
plot2 = diagd %>% ggplot(aes(x=fitted,y=resid)) + geom_point(alpha=0.3) + geom_hline(yintercept=0) + labs(x="Fitted", y="Residuals", caption = 'Figure 3: Fitted v.s. Residuals')
plot2

```

From the figure below, we can find the points have some trends even if they are on the two sides around zero. Thus, it is not very confident to state the variance is constant, which also means there may not exist homoscedasticity.

# 4. Results

- (1) Finally, we build the GLMM model condition on "patient nbr", totally 10 covariates, which is: 

Final Formula $=$ readmitted $\sim(1 \mid$ patient $\mathrm{nbr})+$ race $+$ gender $+$ age $+$ num procedures + number outpatient + number emergency $+$ number diagnoses $+$ insulin $+$ diabetesMed $+$ Length.of.Stay

- (2) After fitting this model on train set, we use it on test set. The AUC is 0.8854 and the $\mathrm{ROC}$ curve is as below (Figure 4). From the accuracy, we can find the predicted results are acceptable and even good. Thus, the model maybe a suitable model for the data.

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# predict Accuracy for test set

final_formula = readmitted ~ (1 | patient_nbr) + race + gender + age + num_procedures + number_outpatient + number_emergency + number_diagnoses + insulin +  diabetesMed + Length.of.Stay

model3_test = glmer(final_formula, data = test_set, family = binomial, nAGQ = 0)

# summary(model3_test)

```

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}

# Prediction for test set

library(pROC)

# Produce the ROC curve. State the AUC value and interpret.

pred_test = predict(model3_test, type = "response") 
roc_para_test = roc(test_set$readmitted ~ pred_test) 
TPR_test = roc_para_test$sensitivities
FPR_test = 1 - roc_para_test$specificities 
# plot

data.frame(
  FPR = FPR_test, 
  TPR = TPR_test
) %>%
  ggplot(mapping = aes(x = FPR, y = TPR)) + 
  geom_path(colour = "red") + 
  geom_abline(slope = 1, intercept = 0, 
              colour = "blue", lty = 2) + 
  geom_text(data.frame(x = 0.7, y = 0.4, 
                       label = paste0("AUC = ", round(auc(roc_para_test),2))),
            mapping = aes(x = x, y = y, label = label)) + 
  labs(caption = "Figure 4")

```

# 5. Discussion

## 5.1. Results Interpretation

The result shows that there are many causes for readmission. The first one is race. If a person is an Asian, he or she is more likely readmitted. The possibility of man readmitted is higher than a woman. Patients who are more than 60 year old but less than 90 years old can be easily readmitted. The higher number of procedures, outpatient , emergency and diagnoses means the higher possibility to get readmitted. The use of insulin is also a crucial reason of readmitted. Moreover, diabetes has positive influence on readmission. In addition, if a patient has already stay long time in hospital, he or she has a greater probability of readmission.

## 5.2. Limitations

Our model gives some guidance to predict the readmission. However, we have several limitations here in our model.

- BIC is a good criteria to select variables. However, it may give too much penalty that leads a under-fit issue.

- No interaction terms are considered in our model (due to the large data set and computing time). We are not sure how this could affect our model. But we believe all these variables cannot be independent with each other. There should be some correlations underneath.

- The model is under linear assumption. We may consider to involve the smooth pattern (model GAM). However, it may make the model too complex.

- There are some problems about stepwise selection method, so it is a meaningful future work on more efficient variable selection methods.

(a)	It yields R-squared values that are badly biased to be high. 

(b)	It yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem. 

(c)	It yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem. 

(d)	The stepwise selection allows us to think too much about statistical model but not the original problem.

\newpage

# Reference















